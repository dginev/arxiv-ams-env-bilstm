{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import _remove_long_seq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input, Dense, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.95, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Assumption: final label choice, setup_labels == \"confusion-envs-v3\":\n",
    "    confusion_map = {\n",
    "        # 0: drop,  # abstract, unclear separation with introduction?\n",
    "        1: 0,  # acknowledgement\n",
    "        # 2: drop, # algorithm, bad data\n",
    "        # 3: drop, # POST2: drop, too correlated with proposition  # POST: assumption, # 0.77 assumption but \"condition\" seems hard to separate\n",
    "        # 4: drop # caption, bad data\n",
    "        5: 1,  # case + proof + step\n",
    "        # 6: drop, # POST2: drop, too correlated with proof, proposition, example  # conclusion + discussion + remark\n",
    "        # 7: drop,  # POST: condition\n",
    "        8: 2,  # lemma + theorem + corollary + proposition + conjecture + fact\n",
    "        9: 2,  # lemma + theorem + corollary + proposition + conjecture + fact\n",
    "        10: 3,  # POST: definition\n",
    "        # 11: drop, #POST2  # conclusion + discussion + remark\n",
    "        12: 4,  # example\n",
    "        13: 2,  # lemma + theorem + corollary + proposition + conjecture + fact\n",
    "        14: 5,  # introduction\n",
    "        15: 2,  # lemma + theorem + corollary + proposition + conjecture + fact\n",
    "        # 16: drop,  # method, too correlated\n",
    "        # 17: drop,  #POST: notation is too noisy / corellated with proof, drop\n",
    "        # 18: drop, other class\n",
    "        # 19: drop, paragraph seems badly separable\n",
    "        20: 6,  # POST: problem\n",
    "        21: 1,  # case + proof + step\n",
    "        22: 2,  # lemma + theorem + corollary + proposition + conjecture + fact\n",
    "        # 23: drop, POST: question too correlated with definition? # problem + question\n",
    "        24: 7,  # related work\n",
    "        # 25: drop, # POST 2  # conclusion + discussion + remark\n",
    "        # 26: drop,  # result, too correlated\n",
    "        27: 1,  # case + proof + step\n",
    "        28: 2,  # lemma + theorem + corollary + proposition + conjecture + fact\n",
    "    }\n",
    "    # Classes v1:\n",
    "    # 0 - acknowledgement\n",
    "    # 1 - assumption = assumption + condition\n",
    "    # 2 - proof = case + proof + step\n",
    "    # 3 - remark = conclusion + discussion + remark\n",
    "    # 4 - proposition = lemma + theorem + corollary + proposition + conjecture + fact\n",
    "    # 5 - definition = definition + notation\n",
    "    # 6 - example\n",
    "    # 7 - introduction\n",
    "    # 8 - problem = problem + question\n",
    "    # 9 - related work\n",
    "    # drop - abstract + algorithm + caption + method + other + paragraph + result\n",
    "\n",
    "    # classes v3:\n",
    "    # 0 - acknowledgement\n",
    "    # 1 - proof = case + proof + step\n",
    "    # 2 - proposition = lemma + theorem + corollary + proposition + conjecture + fact\n",
    "    # 3 - definition\n",
    "    # 4 - example\n",
    "    # 5 - introduction\n",
    "    # 6 - problem\n",
    "    # 7 - related work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-management before main training\n",
    "def load_data(path='data/sandbox_ams_1k.npz',  # _1m\n",
    "              test_split=0.2, seed=521, shuffle=True,\n",
    "              max_per_class=1_000_000,\n",
    "              **kwargs):\n",
    "    \"\"\"Loads the sandbox of AMS paragraph data, as prepared by `sandbox_data_for_keras.py`\n",
    "       and adapts it for use by Keras, with additional preprocessing where needed\n",
    "\n",
    "    # Returns\n",
    "        List containing train-test split of inputs\n",
    "        Four arrays: `x_train, y_train, x_test, y_test`.\n",
    "\n",
    "    Words that were not seen in the training set but are in the test set\n",
    "    have simply been skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    print('-- loading data...')\n",
    "    xs, labels = [], []\n",
    "    with np.load(path) as f:\n",
    "        xs, labels = f['x'].tolist(), f['y'].tolist()\n",
    "    \n",
    "    other_label = len(set(confusion_map.values()))\n",
    "    label_summary = dict.fromkeys(range(0, 28), 0)\n",
    "    print(\"-- reducing to %d label classes\" % (other_label))\n",
    "    iterations = 0\n",
    "    xs_reduced = []\n",
    "    labels_reduced = []\n",
    "    while len(labels) > 0:\n",
    "        iterations += 1\n",
    "        x = xs.pop()\n",
    "        label = labels.pop()\n",
    "        if iterations % 1_000_000 == 0:\n",
    "            print(\"-- %d iterations\" % iterations)\n",
    "        if label in confusion_map:\n",
    "            mapped_label = confusion_map[label]\n",
    "            if label_summary[mapped_label] < max_per_class:\n",
    "                xs_reduced.append(x)\n",
    "                labels_reduced.append(mapped_label)\n",
    "                label_summary[mapped_label] += 1\n",
    "    print(\"-- assigning to arrays\")\n",
    "    xs = np.array(xs_reduced)\n",
    "    xs_reduced = []\n",
    "    labels = np.array(labels_reduced)\n",
    "    labels_reduced = []\n",
    "    gc.collect()\n",
    "    print(\"loaded %d paragagraphs and %d labels\" % (len(xs), len(labels)))\n",
    "\n",
    "    if shuffle:\n",
    "        print(\"-- shuffling data...\")\n",
    "        np.random.seed(seed)\n",
    "        indices = np.arange(len(xs))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        xs = xs[indices]\n",
    "        labels = labels[indices]\n",
    "        gc.collect()\n",
    "\n",
    "    # Might as well report a summary of what is in the labels...      \n",
    "    label_summary = {k: v for k, v in label_summary.items() if v > 0}\n",
    "    print(\"-- Label summary: \", label_summary)\n",
    "\n",
    "    print(\"-- performing train/test cutoff\")\n",
    "    return train_test_split(xs, labels, stratify=labels, test_size=test_split)\n",
    "\n",
    "\n",
    "def get_word_index():\n",
    "    f = open('data/ams_word_index.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    with open('data/ams_word_index.json') as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "    \n",
    "def load_glove():\n",
    "    glove = {}\n",
    "    with open('data/glove.model.txt') as glove_data:\n",
    "        for line in glove_data:\n",
    "            items = line.split()\n",
    "            key = items[0]\n",
    "            glove[key] = np.asarray(items[1:], dtype='float32')\n",
    "    return glove\n",
    "\n",
    "\n",
    "def build_embedding_layer(with_input=False, maxlen=256, vocab_dim=300, mask_zero=True):\n",
    "    print(\"-- loading word embeddings, this may take a little while...\")\n",
    "    index_dict = load_vocab()\n",
    "    word_vectors = load_glove()\n",
    "    # adding 1 to account for 0th index (for masking)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    print(\"-- known dictionary items: \", n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    print(\"-- embeddings \")\n",
    "    if not with_input:\n",
    "        embedding_layer = Embedding(\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False, weights=[embedding_weights])\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        # define inputs here\n",
    "        input_1 = Input(shape=(maxlen,), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            weights=[embedding_weights],\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False)(input_1)\n",
    "        return (embedding_layer, input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading data...\n",
      "-- reducing to 8 label classes\n",
      "-- 1000000 iterations\n",
      "-- 2000000 iterations\n",
      "-- 3000000 iterations\n",
      "-- 4000000 iterations\n",
      "-- 5000000 iterations\n",
      "-- 6000000 iterations\n",
      "-- 7000000 iterations\n",
      "-- assigning to arrays\n",
      "loaded 3058036 paragagraphs and 3058036 labels\n",
      "-- Label summary:  {0: 44997, 1: 1000000, 2: 1000000, 3: 707396, 4: 257868, 5: 15081, 6: 30843, 7: 1851}\n",
      "-- preparing sets...\n",
      "-- performing train/test cutoff\n",
      "x_train shape: (2446428, 480)\n",
      "x_test shape: (611608, 480)\n",
      "y_train shape: (2446428,)\n",
      "y_test shape: (611608,)\n"
     ]
    }
   ],
   "source": [
    "# Eager-Load the data\n",
    "gc.collect()\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_data(path=\"data/sandbox_ams_1m.npz\", shuffle=False)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading word embeddings, this may take a little while...\n",
      "-- known dictionary items:  1000296\n",
      "-- embeddings \n"
     ]
    }
   ],
   "source": [
    "# preparing word embeddings\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "# 08.2018 (subformula lexemes)\n",
    "# Analyzing the arxiv dataset seems to indicate \n",
    "#   a maxlen of 960 is needed to fit 99.2% of the data\n",
    "#   a maxlen of 480 fits 96.03%, and a maxlen of 300 covers 90.0% of paragraphs\n",
    "maxlen = 480\n",
    "embedding_layer = build_embedding_layer(maxlen=maxlen, mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- setting up model layout...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 480, 300)          300088800 \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 480, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 480, 256)          440320    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 480, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 480, 128)          164864    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 480, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, 64)                49664     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 300,744,168\n",
      "Trainable params: 655,368\n",
      "Non-trainable params: 300,088,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM Model Setup\n",
    "n_classes = 8\n",
    "layer_size = 128  # ~maxlen // 4\n",
    "\n",
    "print(\"-- setting up model layout...\")\n",
    "use_dropout = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size // 2, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(CuDNNLSTM(layer_size // 2))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- training model...\n",
      "Train on 1957142 samples, validate on 489286 samples\n",
      "Epoch 1/50\n",
      "1957142/1957142 [==============================] - 3298s 2ms/step - loss: 0.4093 - weighted_sparse_categorical_accuracy: 0.8592 - val_loss: 0.2776 - val_weighted_sparse_categorical_accuracy: 0.9093\n",
      "\n",
      "Epoch 00001: val_weighted_sparse_categorical_accuracy improved from -inf to 0.90934, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 2/50\n",
      "1957142/1957142 [==============================] - 3309s 2ms/step - loss: 0.2714 - weighted_sparse_categorical_accuracy: 0.9116 - val_loss: 0.2487 - val_weighted_sparse_categorical_accuracy: 0.9182\n",
      "\n",
      "Epoch 00002: val_weighted_sparse_categorical_accuracy improved from 0.90934 to 0.91820, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 3/50\n",
      "1957142/1957142 [==============================] - 3306s 2ms/step - loss: 0.2500 - weighted_sparse_categorical_accuracy: 0.9185 - val_loss: 0.2381 - val_weighted_sparse_categorical_accuracy: 0.9219\n",
      "\n",
      "Epoch 00003: val_weighted_sparse_categorical_accuracy improved from 0.91820 to 0.92188, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 4/50\n",
      "1957142/1957142 [==============================] - 3312s 2ms/step - loss: 0.2392 - weighted_sparse_categorical_accuracy: 0.9219 - val_loss: 0.2324 - val_weighted_sparse_categorical_accuracy: 0.9243\n",
      "\n",
      "Epoch 00004: val_weighted_sparse_categorical_accuracy improved from 0.92188 to 0.92429, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 5/50\n",
      "1957142/1957142 [==============================] - 3311s 2ms/step - loss: 0.2323 - weighted_sparse_categorical_accuracy: 0.9241 - val_loss: 0.2323 - val_weighted_sparse_categorical_accuracy: 0.9246\n",
      "\n",
      "Epoch 00005: val_weighted_sparse_categorical_accuracy improved from 0.92429 to 0.92460, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 6/50\n",
      "1957142/1957142 [==============================] - 3314s 2ms/step - loss: 0.2276 - weighted_sparse_categorical_accuracy: 0.9256 - val_loss: 0.2293 - val_weighted_sparse_categorical_accuracy: 0.9257\n",
      "\n",
      "Epoch 00006: val_weighted_sparse_categorical_accuracy improved from 0.92460 to 0.92572, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 7/50\n",
      "1957142/1957142 [==============================] - 3312s 2ms/step - loss: 0.2239 - weighted_sparse_categorical_accuracy: 0.9269 - val_loss: 0.2304 - val_weighted_sparse_categorical_accuracy: 0.9253\n",
      "\n",
      "Epoch 00007: val_weighted_sparse_categorical_accuracy did not improve from 0.92572\n",
      "Epoch 8/50\n",
      "1957142/1957142 [==============================] - 3312s 2ms/step - loss: 0.2220 - weighted_sparse_categorical_accuracy: 0.9274 - val_loss: 0.2273 - val_weighted_sparse_categorical_accuracy: 0.9264\n",
      "\n",
      "Epoch 00008: val_weighted_sparse_categorical_accuracy improved from 0.92572 to 0.92644, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 9/50\n",
      "1957142/1957142 [==============================] - 3310s 2ms/step - loss: 0.2201 - weighted_sparse_categorical_accuracy: 0.9279 - val_loss: 0.2266 - val_weighted_sparse_categorical_accuracy: 0.9268\n",
      "\n",
      "Epoch 00009: val_weighted_sparse_categorical_accuracy improved from 0.92644 to 0.92679, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 10/50\n",
      "1957142/1957142 [==============================] - 3308s 2ms/step - loss: 0.2176 - weighted_sparse_categorical_accuracy: 0.9287 - val_loss: 0.2271 - val_weighted_sparse_categorical_accuracy: 0.9272\n",
      "\n",
      "Epoch 00010: val_weighted_sparse_categorical_accuracy improved from 0.92679 to 0.92719, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "Epoch 11/50\n",
      "1957142/1957142 [==============================] - 3309s 2ms/step - loss: 0.2166 - weighted_sparse_categorical_accuracy: 0.9291 - val_loss: 0.2260 - val_weighted_sparse_categorical_accuracy: 0.9270\n",
      "\n",
      "Epoch 00011: val_weighted_sparse_categorical_accuracy did not improve from 0.92719\n",
      "Epoch 12/50\n",
      "1957142/1957142 [==============================] - 3314s 2ms/step - loss: 0.2168 - weighted_sparse_categorical_accuracy: 0.9289 - val_loss: 0.2246 - val_weighted_sparse_categorical_accuracy: 0.9272\n",
      "\n",
      "Epoch 00012: val_weighted_sparse_categorical_accuracy did not improve from 0.92719\n",
      "Epoch 13/50\n",
      "1957142/1957142 [==============================] - 3310s 2ms/step - loss: 0.2150 - weighted_sparse_categorical_accuracy: 0.9296 - val_loss: 0.2243 - val_weighted_sparse_categorical_accuracy: 0.9275\n",
      "\n",
      "Epoch 00013: val_weighted_sparse_categorical_accuracy improved from 0.92719 to 0.92749, saving model to bilstm128_batch64_cat8_gpu-checkpoint.h5\n",
      "-- saving model to disk : bilstm128_batch64_cat8_gpu \n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model_file = \"bilstm%d_batch%d_cat%d_gpu\" % (\n",
    "    layer_size, batch_size, n_classes)\n",
    "\n",
    "# Perform training\n",
    "print('-- training model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          class_weight=class_weights,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          callbacks=[checkpoint, earlystop],\n",
    "          validation_split=0.2)\n",
    "# serialize model to JSON\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class test measures:\n",
      "611608/611608 [==============================] - 398s 650us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9000\n",
      "           1       0.95      0.95      0.95    200000\n",
      "           2       0.92      0.95      0.94    200000\n",
      "           3       0.93      0.94      0.93    141479\n",
      "           4       0.86      0.75      0.80     51574\n",
      "           5       0.84      0.76      0.80      3016\n",
      "           6       0.86      0.82      0.84      6169\n",
      "           7       0.72      0.80      0.76       370\n",
      "\n",
      "   micro avg       0.93      0.93      0.93    611608\n",
      "   macro avg       0.88      0.87      0.88    611608\n",
      "weighted avg       0.93      0.93      0.93    611608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(x_test, verbose=1, batch_size=batch)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class test measures:\n",
    "```\n",
    "        611608/611608 [==============================] - 398s 650us/step\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "acknowledgement       1.00      1.00      1.00      9000\n",
    "          proof       0.95      0.95      0.95    200000\n",
    "    proposition       0.92      0.95      0.94    200000\n",
    "     definition       0.93      0.94      0.93    141479\n",
    "        example       0.86      0.75      0.80     51574\n",
    "   introduction       0.84      0.76      0.80      3016\n",
    "        problem       0.86      0.82      0.84      6169\n",
    "   related work       0.72      0.80      0.76       370\n",
    "\n",
    "   micro avg       0.93      0.93      0.93    611608\n",
    "   macro avg       0.88      0.87      0.88    611608\n",
    "weighted avg       0.93      0.93      0.93    611608\n",
    "```\n",
    "\n",
    "Trained on 2.44 million paragraphs and evaluated on a test set of 0.6 million paragraphs, for a weighted 0.93 F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
