{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "import threading\n",
    "import time\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import _remove_long_seq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input, Dense, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# training on 1080 Ti, you may want to adjust these for your own hardware\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.65, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index():\n",
    "    f = open('data/word_index.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    with open('data/word_index_2019.json') as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "    \n",
    "def load_glove():\n",
    "    glove = {}\n",
    "    with open('data/glove.arxmliv.15B.300d.txt') as glove_data:\n",
    "        for line in glove_data:\n",
    "            items = line.split()\n",
    "            key = items[0]\n",
    "            glove[key] = np.asarray(items[1:], dtype='float32')\n",
    "    return glove\n",
    "\n",
    "\n",
    "def build_embedding_layer(with_input=False, maxlen=512, vocab_dim=300, mask_zero=True):\n",
    "    print(\"-- loading word embeddings, this may take a couple of minutes...\")\n",
    "    index_dict = load_vocab()\n",
    "    word_vectors = load_glove()\n",
    "    # adding 1 to account for 0th index (for masking)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    print(\"-- known dictionary items: \", n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    print(\"-- embeddings \")\n",
    "    if not with_input:\n",
    "        embedding_layer = Embedding(\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False, weights=[embedding_weights])\n",
    "        return embedding_layer\n",
    "    else:\n",
    "        # define inputs here\n",
    "        input_1 = Input(shape=(maxlen,), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            weights=[embedding_weights],\n",
    "            mask_zero=mask_zero,\n",
    "            output_dim=vocab_dim, input_dim=n_symbols, input_length=maxlen, trainable=False)(input_1)\n",
    "        return (embedding_layer, input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data is obtained via batch loading from HDF5\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, batch_size=128, mode=\"train\", dim=512,\n",
    "             n_classes=46, x_hf = None, y_hf = None, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.x_hf = x_hf\n",
    "        self.y_hf = y_hf\n",
    "        self.mode = mode\n",
    "        self.total_len = self.y_hf.shape[0]\n",
    "        self.validation_len = int(np.ceil(0.1 * self.total_len))\n",
    "        self.training_len = self.total_len - self.validation_len\n",
    "        if self.mode == \"validation\":\n",
    "            self.data_len = self.validation_len\n",
    "            self.list_IDs = np.arange(self.training_len, self.total_len)\n",
    "        else:\n",
    "            self.data_len = self.training_len\n",
    "            self.list_IDs = np.arange(0,self.training_len+1)\n",
    "            \n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end() \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples,  dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim), dtype=int)\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self.x_hf[ID]\n",
    "            # Store class\n",
    "            y[i] = self.y_hf[ID]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size = 128\n",
    "# Parameters\n",
    "data_hf = h5py.File(\"data/statement_paragraphs_arxmliv_08_2019.hdf5\", 'r')\n",
    "generator_params = {\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 46,\n",
    "    'shuffle': False,\n",
    "    'x_hf': data_hf['x_train'],\n",
    "    'y_hf': data_hf['y_train']\n",
    "}\n",
    "# Generators\n",
    "training_generator = DataGenerator(**generator_params)\n",
    "validation_generator = DataGenerator(mode=\"validation\", **generator_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.41149278888023     0.7057244877771286   3.9829870906107514\n",
      " 36.374054954547326   11.892902773743046   13.80218842868916\n",
      "  0.06770583774558957 31.914481685860814    5.063662428479154\n",
      "  0.9402773356164806  10.419343112996348    7.468410273301577\n",
      " 16.455819246257715    0.9736470848601226  45.38169981935378\n",
      " 49.34844832108397     0.5689703684124646  19.235229180986853\n",
      " 18.96762510456661     2.494897996609856    1.2315613896598328\n",
      "  3.982038288896814   23.05413209027794    39.18802985114313\n",
      " 44.29421208174715     0.4550588490658705   0.3176264217702254\n",
      "  4.007815626539546    1.3989257586172257  28.458457503298686\n",
      "  6.908286212483653    6.805206663531961    6.996028838881382\n",
      "  3.784637564176529    0.17672347524556367  7.361449047820426\n",
      "  0.5111017831551758  12.082011214554875    8.752362066781215\n",
      "  0.602249639517737    1.6020207292277908   8.091288609751567\n",
      " 33.713939045033776    3.439557710409422    0.31825111426082914\n",
      " 66.8882673746634    ]\n",
      "-- loading word embeddings, this may take a couple of minutes...\n",
      "-- known dictionary items:  989137\n",
      "-- embeddings \n"
     ]
    }
   ],
   "source": [
    "# It takes X minutes to recompute, memoizing here for the 46 class variant:\n",
    "# class_weights = compute_class_weight('balanced', np.unique(training_generator.y_hf), training_generator.y_hf)\n",
    "# np.set_printoptions(precision=32, suppress=True)\n",
    "# print(class_weights)\n",
    "class_weights = [0.41149278888023,     0.7057244877771286,   3.9829870906107514\n",
    " 36.374054954547326   11.892902773743046   13.80218842868916\n",
    "  0.06770583774558957 31.914481685860814    5.063662428479154\n",
    "  0.9402773356164806  10.419343112996348    7.468410273301577\n",
    " 16.455819246257715    0.9736470848601226  45.38169981935378\n",
    " 49.34844832108397     0.5689703684124646  19.235229180986853\n",
    " 18.96762510456661     2.494897996609856    1.2315613896598328\n",
    "  3.982038288896814   23.05413209027794    39.18802985114313\n",
    " 44.29421208174715     0.4550588490658705   0.3176264217702254\n",
    "  4.007815626539546    1.3989257586172257  28.458457503298686\n",
    "  6.908286212483653    6.805206663531961    6.996028838881382\n",
    "  3.784637564176529    0.17672347524556367  7.361449047820426\n",
    "  0.5111017831551758  12.082011214554875    8.752362066781215\n",
    "  0.602249639517737    1.6020207292277908   8.091288609751567\n",
    " 33.713939045033776    3.439557710409422    0.31825111426082914\n",
    " 66.8882673746634    ]\n",
    "\n",
    "maxlen = 512\n",
    "# Preparing word embeddings\n",
    "embedding_layer = build_embedding_layer(maxlen=maxlen, mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- setting up model layout...\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3721: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 512, 300)          296741100 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512, 256)          440320    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512, 128)          164864    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 64)                49664     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 297,398,938\n",
      "Trainable params: 657,838\n",
      "Non-trainable params: 296,741,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM Model Setup\n",
    "n_classes = 46\n",
    "layer_size = 128  # ~maxlen // 4\n",
    "\n",
    "print(\"-- setting up model layout...\")\n",
    "use_dropout = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(layer_size // 2, return_sequences=True)))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(CuDNNLSTM(layer_size // 2))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              weighted_metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"confusion_bilstm%d_batch%d_cat%d_gpu\" % (\n",
    "    layer_size, batch_size, n_classes)\n",
    "\n",
    "# Checkpoints: 1) save best model at epoch end, 2) stop early when metric stops improving\n",
    "checkpoint = ModelCheckpoint(model_file+\"-checkpoint.h5\",\n",
    "                             monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_weighted_sparse_categorical_accuracy',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          \n",
    "                          restore_best_weights=True,\n",
    "                          verbose=0, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- training model...\n",
      "WARNING:tensorflow:From /home/deyan/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "124353/124353 [==============================] - 19642s 158ms/step - loss: 0.8034 - weighted_sparse_categorical_accuracy: 0.7493 - val_loss: 0.7821 - val_weighted_sparse_categorical_accuracy: 0.7522\n",
      "\n",
      "Epoch 00001: val_weighted_sparse_categorical_accuracy improved from -inf to 0.75216, saving model to confusion_bilstm128_batch128_cat46_gpu-checkpoint.h5\n",
      "Epoch 2/50\n",
      " 76154/124353 [=================>............] - ETA: 1:59:21 - loss: 0.7228 - weighted_sparse_categorical_accuracy: 0.7694"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "print('-- training model...')\n",
    "# TODO: How can we make this work with more workers? HDF5 is not thread-safe for reads...\n",
    "# maybe use the unpacked .txt files and map them through the dictionary each time? Unsure... \n",
    "# On a single CPU worker: 2.5 hours per epoch for the full data, with the main BiLSTM model.\n",
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    workers = 1,\n",
    "    use_multiprocessing=False,\n",
    "    class_weight=class_weights,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, earlystop])\n",
    "\n",
    "print(\"-- saving model to disk : %s \" % model_file)\n",
    "model.save(model_file+'_notebook.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "# model = load_model(model_file+\"_notebook.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-class test measures:\")\n",
    "y_pred = model.predict_classes(data_hf['x_test'], verbose=1, batch_size=batch_size)\n",
    "print(classification_report(data_hf['y_test'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=np.array(sorted([\n",
    "    \"abstract\", \"acknowledgement\", \"analysis\", \"application\", \"assumption\",\n",
    "    \"background\", \"caption\", \"case\", \"claim\", \"conclusion\", \"condition\",\n",
    "    \"conjecture\", \"contribution\", \"corollary\", \"data\", \"dataset\",\n",
    "    \"definition\", \"demonstration\", \"description\", \"discussion\", \"example\",\n",
    "    \"experiment\", \"fact\", \"future work\", \"implementation\", \"introduction\",\n",
    "    \"lemma\", \"methods\", \"model\", \"motivation\", \"notation\", \"observation\",\n",
    "    \"preliminaries\", \"problem\", \"proof\", \"property\", \"proposition\",\n",
    "    \"question\", \"related work\", \"remark\", \"result\", \"simulation\", \"step\",\n",
    "    \"summary\", \"theorem\", \"theory\", ]))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n",
    "        annot = True\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        annot = False\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(50,40))\n",
    "    df_cm = pd.DataFrame(cm, index = classes,\n",
    "                  columns = classes)\n",
    "    sn.set(font_scale=1.4)#for label size\n",
    "    sn.heatmap(df_cm, annot=annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(data_hf['y_test'], y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class test measures:\n",
    "\n",
    "Using all 17.6 million statements for training (converged in X epochs), then testing on the unseen test set of 4.4 million entries:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Confusion Matrix Analysis\n",
    "\n",
    "Strong true signal:\n",
    " - acknowledgement - 1.0\n",
    " - abstract - 0.95\n",
    " - proof - 0.94\n",
    " - definition - 0.91\n",
    " - introduction - 0.9\n",
    " - question - 0.84\n",
    " - keywords - 0.84\n",
    " - remark - 0.74 (proof 0.12)\n",
    " - result - 0.74\n",
    " - conclusion - 0.73 (abstract 0.12)\n",
    "\n",
    "Mid-tier true signal:\n",
    " - lemma - 0.72 (theorem 0.21)\n",
    " - theorem - 0.62 (lemma 0.31)\n",
    " - related work - 0.63 (0.23 introduction)\n",
    " - example - 0.62 (0.1 proof + 0.1 remark)\n",
    " - method - 0.49 (result 0.27)\n",
    " - problem - 0.47 (question 0.31)\n",
    "\n",
    "Strong confused signal:\n",
    " - affirmation -> lemma\n",
    " - comment -> 0.72 remark\n",
    " - demonstration -> 0.8 proof\n",
    " - expansion -> 1.0 theorem\n",
    " - expectation -> 0.5 introduction + 0.5 remark\n",
    " - explanation -> 0.33 proof + 0.67 remark\n",
    " - hint -> 1.0 proof\n",
    " - issue -> 1.0 question\n",
    "\n",
    "Weak Confused signal:\n",
    " - answer -> remark + proof + example\n",
    " - assumption -> 0.37 self + 0.39 lemma + 0.1 theorem\n",
    " - bound -> 0.44 lemma + 0.56 theorem\n",
    " - case -> lemma + proof\n",
    " - claim -> 0.62 lemma + 0.21 theorem\n",
    " - condition -> 0.49 lemma + 0.25 assumption\n",
    " - conjecture -> 0.57 theorem + 0.29 lemma\n",
    " - constraint -> definition + proof + lemma\n",
    " - convention -> remark + definition\n",
    " - corollary -> 0.43 lemma + 0.46 theorem\n",
    " - criterion -> lemma + theorem\n",
    " - discussion -> 0.4 conclusion +0.28 self + 0.13 result + 0.08 introduction\n",
    " - exercise -> problem + example + result\n",
    " - experiment -> 0.33 example + 0.37 result\n",
    " - fact - 0.56 lemma + 0.3 theorem\n",
    " - note - 0.6 remark+ 0.13 proof\n",
    " - notation -> 0.49 definition + 0.15 proof + 0.11 remark\n",
    " - proposition -> 0.52 lemma + 0.36 theorem + 0.06 self\n",
    " - rule -> self, lemma, proof, definition\n",
    " - observation -> 0.41 lemma + 0.24 remark + 0.15 theorem + 0.11 proof\n",
    " - overview -> 0.26 introduction + 0.25 result\n",
    " - principle -> theorem + lemma + definition + remark\n",
    " - solution -> example, proof, lemma, theorem\n",
    " - step -> proof,lemma,definition\n",
    " - summary -> 0.39 theorem, 0.22 result\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "I. **Strategy for a \"confusion-free\" classification scheme**\n",
    "  - preserve strong true signal classes\n",
    "    - acknowledgement, abstract, proof, definition, introduction, question, keywords, remark, conclusion, result\n",
    "  - drop any weak confused signal class based on low volume data (< 0.5% of data for biggest class, ~ 10,000 paragraphs)\n",
    "    - notice, expansion, hint, expectation, explanation, affirmation, answer, issue, bound, summary, experiment,\n",
    "    - solution, criterion, principle, comment, exercise, constraint, rule, convention, case,\n",
    "    - step, overview,\n",
    "  - for bigger classes, drop if not combinable in a meaningful way:\n",
    "    - drop: notation, observation, method,\n",
    "  - merge together related mid-tier true signals WHEN related semantically (theorem, lemma)\n",
    "  - add to merged classes stronger, or clearly interpretable signals that would become strong true signals when added.\n",
    "    - BIG \"proposition\" class is clearly motivated, all constituents misclassify to \"theorem+lemma\":\n",
    "      - proposition = lemma + theorem + proposition + assumption + condition + fact + conjecture + claim + corollary\n",
    "    - Enhanced \"remark\" = remark + note\n",
    "    - Enhanced \"proof\" = proof + demonstration\n",
    "    - Enhanced \"question\" = question + problem\n",
    "    - Enhanced \"conclusion\" = conclusion + discussion\n",
    "    \n",
    "  - \"example\" is a bit on the fence. Keep, as it has 50,000 entries.\n",
    "  - \"related work\" is also on the fence. Keep, as it has 50,000 entries.\n",
    "\n",
    "  **Final assembly**: 24 of the original classes grouped into 13 strongly separable unions:\n",
    "\n",
    "| Class           | Additional Members | Frequency |\n",
    "|:----------------|:-------------------|----------:|\n",
    "| abstract        | -                  | 1,030,774 |\n",
    "| acknowledgement | -                  |   162,230 |\n",
    "| conclusion      | discussion         |   401,235 |\n",
    "| definition      | -                  |   686,717 |\n",
    "| example         | -                  |   295,152 |\n",
    "| introduction    | -                  |   688,530 |\n",
    "| keywords        | -                  |     1,565 |\n",
    "| proof           | demonstration      | 2,148,793 |\n",
    "| proposition     | assumption, claim, | 4,060,029 |\n",
    "| +               | condition,         |         + |\n",
    "| +               | conjecture,        |         + |\n",
    "| +               | corollary, fact,   |         + |\n",
    "| +               | lemma, theorem     |         + |           \n",
    "| problem         | question           |    57,609 |\n",
    "| related work    | -                  |    26,299 |\n",
    "| remark          | note               |   643,500 |\n",
    "| result          | -                  |   239,931 |\n",
    "\n",
    "Frequency reported includes the entire 10.5 million paragraph dataset.\n",
    "\n",
    "  Dropped (25) =\n",
    "  notice, expansion, hint, expectation, explanation, affirmation, answer, issue, bound, summary, experiment,\n",
    "  solution, criterion, principle, comment, exercise, constraint, rule, convention, case, step, overview, notation, observation, method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
